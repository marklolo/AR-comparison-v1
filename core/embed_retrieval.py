"""
å‘é‡æª¢ç´¢æ¨¡çµ„
æ”¯æ´æ–‡æœ¬åµŒå…¥å’Œå‘é‡æª¢ç´¢åŠŸèƒ½
"""
import os
from typing import List, Dict, Optional
from dotenv import load_dotenv

try:
    import numpy as np
except ImportError:
    np = None

try:
    import faiss
except ImportError:
    faiss = None
    
try:
    import google.generativeai as genai
except ImportError:
    genai = None

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()

class EmbeddingService:
    def __init__(self):
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model_name = 'models/text-embedding-004'
        
        if not self.api_key or genai is None:
            print("è­¦å‘Š: æœªè¨­ç½® GEMINI_API_KEY æˆ–æœªå®‰è£ google-generativeaiï¼Œå‘é‡æª¢ç´¢åŠŸèƒ½å°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
            self.client = None
        else:
            genai.configure(api_key=self.api_key)
            self.client = genai
    
    def is_available(self) -> bool:
        """æª¢æŸ¥åµŒå…¥æœå‹™æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def embed_text(self, text: str) -> Optional[np.ndarray]:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        """
        if not self.is_available():
            return self._generate_mock_embedding(text)
        
        try:
            result = genai.embed_content(
                model=self.model_name,
                content=text,
                task_type="retrieval_document"
            )
            
            embedding = np.array(result['embedding'], dtype=np.float32)
            return embedding
            
        except Exception as e:
            print(f"åµŒå…¥ç”Ÿæˆå¤±æ•—: {e}")
            return self._generate_mock_embedding(text)
    
    def embed_query(self, query: str) -> Optional[np.ndarray]:
        """
        ç”ŸæˆæŸ¥è©¢åµŒå…¥å‘é‡
        """
        if not self.is_available():
            return self._generate_mock_embedding(query)
        
        try:
            result = genai.embed_content(
                model=self.model_name,
                content=query,
                task_type="retrieval_query"
            )
            
            embedding = np.array(result['embedding'], dtype=np.float32)
            return embedding
            
        except Exception as e:
            print(f"æŸ¥è©¢åµŒå…¥ç”Ÿæˆå¤±æ•—: {e}")
            return self._generate_mock_embedding(query)
    
    def embed_batch(self, texts: List[str]) -> List[np.ndarray]:
        """
        æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        """
        embeddings = []
        
        for text in texts:
            embedding = self.embed_text(text)
            if embedding is not None:
                embeddings.append(embedding)
            else:
                embeddings.append(np.zeros(768, dtype=np.float32))
        
        return embeddings
    
    def _generate_mock_embedding(self, text: str, dim: int = 768) -> np.ndarray:
        """
        ç”Ÿæˆæ¨¡æ“¬åµŒå…¥å‘é‡ï¼ˆåŸºæ–¼æ–‡æœ¬å“ˆå¸Œï¼‰
        """
        # ä½¿ç”¨æ–‡æœ¬å“ˆå¸Œç”Ÿæˆç¢ºå®šæ€§çš„æ¨¡æ“¬å‘é‡
        np.random.seed(hash(text) % (2**32))
        embedding = np.random.normal(0, 1, dim).astype(np.float32)
        # æ­£è¦åŒ–
        embedding = embedding / np.linalg.norm(embedding)
        return embedding

class VectorRetriever:
    def __init__(self, embedding_service: EmbeddingService):
        self.embedding_service = embedding_service
        self.index = None
        self.chunks = []
        self.dimension = 768
    
    def build_index(self, chunks: List[Dict]) -> bool:
        """
        å»ºç«‹å‘é‡ç´¢å¼•
        """
        if not chunks:
            print("æ²’æœ‰å…§å®¹ç”¨æ–¼å»ºç«‹ç´¢å¼•")
            return False
        
        print(f"æ­£åœ¨ç‚º {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µå»ºç«‹å‘é‡ç´¢å¼•...")
        
        # æå–æ–‡æœ¬
        texts = [chunk['text'] for chunk in chunks]
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.embedding_service.embed_batch(texts)
        
        if not embeddings:
            print("åµŒå…¥å‘é‡ç”Ÿæˆå¤±æ•—")
            return False
        
        # å»ºç«‹ FAISS ç´¢å¼•
        embeddings_matrix = np.array(embeddings).astype(np.float32)
        self.dimension = embeddings_matrix.shape[1]
        
        if faiss is not None:
            self.index = faiss.IndexFlatIP(self.dimension)
            faiss.normalize_L2(embeddings_matrix)
            self.index.add(embeddings_matrix)
        
        # ä¿å­˜æ–‡æª”ç‰‡æ®µ
        self.chunks = chunks
        
        print(f"å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆ")
        return True
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        åŸ·è¡Œå‘é‡æª¢ç´¢
        """
        if self.index is None or not self.chunks:
            print("å‘é‡ç´¢å¼•æœªå»ºç«‹")
            return []
        
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        query_embedding = self.embedding_service.embed_query(query)
        if query_embedding is None:
            print("æŸ¥è©¢å‘é‡ç”Ÿæˆå¤±æ•—")
            return []
        
        # æ­£è¦åŒ–æŸ¥è©¢å‘é‡
        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)
        if faiss is not None:
            faiss.normalize_L2(query_embedding)
        
        results = []
        
        if faiss is not None and self.index is not None:
            # åŸ·è¡Œæœç´¢
            scores, indices = self.index.search(query_embedding, min(top_k, len(self.chunks)))
            
            for score, idx in zip(scores[0], indices[0]):
                if idx >= len(self.chunks):
                    continue
                
                chunk = self.chunks[idx]
                result = {
                    'chunk_id': chunk['chunk_id'],
                    'text': chunk['text'],
                    'page_ref': chunk.get('page_ref', ''),
                    'doc_id': chunk.get('doc_id', ''),
                    'similarity_score': float(score),
                    'section_type': chunk.get('section_type', 'text')
                }
                results.append(result)
        
        return results

class SemanticSearchEngine:
    """
    èªžç¾©æœç´¢å¼•æ“Žä¸»é¡ž
    """
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.retriever = VectorRetriever(self.embedding_service)
    
    def initialize_from_reports(self, reports: List[Dict]) -> bool:
        """
        å¾žå ±å‘Šåˆå§‹åŒ–æœç´¢å¼•æ“Ž
        """
        all_chunks = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æª”ç‰‡æ®µ
        for report in reports:
            if hasattr(report, 'sections'):
                from core.parse_pdf import PDFParser
                parser = PDFParser()
                chunks = parser.slice_content_for_search(report)
                all_chunks.extend(chunks)
        
        # å»ºç«‹ç´¢å¼•
        return self.retriever.build_index(all_chunks)
    
    def search_semantic(self, query: str, top_k: int = 10) -> Dict[str, List[Dict]]:
        """
        åŸ·è¡Œèªžç¾©æœç´¢ï¼Œè¿”å›žæŒ‰å…¬å¸åˆ†çµ„çš„çµæžœ
        """
        print(f"ðŸ” åŸ·è¡Œèªžç¾©æœç´¢: '{query}'")
        
        # åŸ·è¡Œæœç´¢
        results = self.retriever.search(query, top_k)
        
        if not results:
            return {}
        
        # æŒ‰å…¬å¸åˆ†çµ„çµæžœ
        grouped_results = {}
        for result in results:
            company = result.get('doc_id', '').split('_')[0] if result.get('doc_id') else 'unknown'
            if company not in grouped_results:
                grouped_results[company] = []
            grouped_results[company].append(result)
        
        return grouped_results
    
    def is_ready(self) -> bool:
        """
        æª¢æŸ¥æœç´¢å¼•æ“Žæ˜¯å¦å°±ç·’
        """
        return (self.retriever.index is not None and 
                len(self.retriever.chunks) > 0)